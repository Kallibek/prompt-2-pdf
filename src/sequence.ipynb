{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a39445b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Imports and environment\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from markdown_pdf import MarkdownPdf, Section\n",
    "\n",
    "# Load environment variables (e.g. OPENAI_API_KEY)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bba1cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # User‐configurable variables\n",
    "\n",
    "# 1. Define the topic you want to learn\n",
    "TOPIC = \"AI Engineering to build ai powered apps and agents\"            # e.g. \"Apache Iceberg\"\n",
    "\n",
    "# 2. Define how many parts to split into\n",
    "N_PARTS = 20                    # e.g. 10 parts\n",
    "\n",
    "# 3. Output PDF configuration\n",
    "output_file = \"../results/AI_Engineering_by_ChatGPT.pdf\"\n",
    "model = \"gpt-4.1-mini\"\n",
    "toc_level = 3\n",
    "optimize = False\n",
    "use_web_search = True\n",
    "verbose = True\n",
    "max_output_tokens = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec05fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function definitions\n",
    "\n",
    "def generate_markdown_from_prompt(current_prompt: str,\n",
    "                                  prev_prompt_results: list[dict],\n",
    "                                  client) -> tuple[str, list[dict]]:\n",
    "    \"\"\"\n",
    "    - current_prompt: new user instruction (\"Now generate part i only.\")\n",
    "    - prev_prompt_results: list of prior messages [{\"role\": \"...\", \"content\": \"...\"}]\n",
    "    - client: OpenAI client\n",
    "\n",
    "    Returns: (assistant_markdown, updated_history_list)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Copy prior history and append the new user message\n",
    "        messages = prev_prompt_results.copy()\n",
    "        messages.append({\"role\": \"user\", \"content\": current_prompt})\n",
    "\n",
    "        # Decide whether to use web-search tool\n",
    "        tools = [{\"type\": \"web_search_preview\"}] if use_web_search else []\n",
    "\n",
    "        # Call the API with the entire history\n",
    "        response = client.responses.create(\n",
    "            model=model,\n",
    "            tools=tools,\n",
    "            input=messages,\n",
    "            max_output_tokens=max_output_tokens\n",
    "        )\n",
    "\n",
    "        # Extract only the assistant’s new reply text\n",
    "        assistant_reply = response.output_text.strip()\n",
    "\n",
    "        # Append assistant’s message to history so next call has full context\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "        # Return the \"Prompt: ...\" wrapper plus the reply, and the updated history\n",
    "        return (\n",
    "            f\"Prompt: {current_prompt}\\n\\n\" + assistant_reply,\n",
    "            messages\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"AI request failed for prompt '{current_prompt}': {e}\")\n",
    "        # Even on failure, return the unmodified history so we don’t lose context\n",
    "        return f\"{current_prompt}\\n\\n---\\n\\n\", prev_prompt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce8516ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting part 0/20…\n",
      "Requesting part 1/20…\n",
      "Requesting part 2/20…\n",
      "Requesting part 3/20…\n",
      "Requesting part 4/20…\n",
      "Requesting part 5/20…\n",
      "Requesting part 6/20…\n",
      "Requesting part 7/20…\n",
      "Requesting part 8/20…\n",
      "Requesting part 9/20…\n",
      "Requesting part 10/20…\n",
      "Requesting part 11/20…\n",
      "Requesting part 12/20…\n",
      "Requesting part 13/20…\n",
      "Requesting part 14/20…\n",
      "Requesting part 15/20…\n",
      "Requesting part 16/20…\n",
      "Requesting part 17/20…\n",
      "Requesting part 18/20…\n",
      "Requesting part 19/20…\n",
      "Requesting part 20/20…\n"
     ]
    }
   ],
   "source": [
    "# # Generate all parts with context and build PDF\n",
    "\n",
    "# Read custom CSS if any\n",
    "css = Path(\"custom.css\").read_text(encoding=\"utf-8\")\n",
    "pdf = MarkdownPdf(toc_level=toc_level, optimize=optimize)\n",
    "\n",
    "# Verify API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"Environment variable OPENAI_API_KEY is not set.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Instantiate client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Initialize history with a single system instruction\n",
    "history_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"Provide output in Markdown. \"\n",
    "            \"Use **bold** text for headers and do not use '#' headers. \"\n",
    "            \"Don't add line separators in the response.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# Loop through each part\n",
    "for part_idx in range(0, N_PARTS + 1):\n",
    "    if part_idx == 0:\n",
    "        # First prompt has no prior assistant replies\n",
    "        current_prompt = f\"Teach me {TOPIC} in {N_PARTS} prompts. Now generate the prompts only.\"\n",
    "        section = Section(\"# \" + current_prompt)\n",
    "        pdf.add_section(section, user_css=css)\n",
    "    else:\n",
    "        # Subsequent prompts rely on history_messages\n",
    "        current_prompt = f\"Now generate response for prompt # {part_idx} only.\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Requesting part {part_idx}/{N_PARTS}…\")\n",
    "\n",
    "    # Call helper, passing current_prompt + accumulated history\n",
    "    md_content, history_messages = generate_markdown_from_prompt(\n",
    "        current_prompt,\n",
    "        history_messages,\n",
    "        client\n",
    "    )\n",
    "\n",
    "    # Add returned Markdown as a new PDF section\n",
    "    section = Section(md_content)\n",
    "    pdf.add_section(section, user_css=css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d83089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF successfully saved to ../results/AI_Engineering_by_ChatGPT.pdf\n"
     ]
    }
   ],
   "source": [
    "# # Save as PDF\n",
    "try:\n",
    "    pdf.save(output_file)\n",
    "    print(f\"PDF successfully saved to {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save PDF: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7d5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa82be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "081c2001",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
